from src.experiment.prepare_data import process_data
from src.experiment.finetuner import finetune
from src.experiment.eval import evaluate_model

import os
import torch
import torch.multiprocessing as mp
import argparse
import numpy as np

def get_args():
    parser = argparse.ArgumentParser(description="Full experiment pipeline. Data processing, finetuning, and evaluation.")
    # shared args
    parser.add_argument('--pipe', '-p', type=str, choices=['data', 'finetune', 'evaluate', 'complete'], help="Which part of pipeline to run. Options are: data, finetune, evaluate, complete.")
    parser.add_argument('--splits_path', '-s', type=str, help="Folder path to save test/train/val splits (when using data pipe), or to load the splits from (when using finetune pipe).")
    parser.add_argument('--c_input_path', '-c', type=str, help="Long-CLIP checkpoint path to finetune when using finetune pipe, or Long-CLIP checkpoint to eval when using evaluate pipe.")
    parser.add_argument('--corrupted', '-x', type=str, help="Path to the .txt file that contains missing or corrupted images (.txt generated by data pipe).")

    # data pipe exclusive args
    parser.add_argument('--metadata_path', '-m', type=str, help="Path to metadata tsv file.")
    parser.add_argument('--images_path', '-i', type=str, help="Path to images folder.")

    # finetune pipe exclusive args
    parser.add_argument('--distributed', '-d', action="store_true", help="Multi-gpu fine-tuning.")
    parser.add_argument('--batch_size', '-b', type=int, default=30, help="Fine-tuning batch_size, defaults to 30.")
    parser.add_argument('--c_output_path', '-o', type=str, help="Folder path to save the checkpoints and logs generated by finetune pipe.")

    # evaluate pipe exclusive args
    parser.add_argument('--test_split_path', '-t', type=str, help="Path to test split .npy file (generated by data pipe).")
    parser.add_argument('--return_mean', '-r', action='store_true', default=True, help="Defaults to True. If True, returns mean of each metric. If False, returns dict with metric for each query.")
    parser.add_argument('--qrel_input_path', '-qi', type=str, help="(Optional) Qrel .json path to load in. If no path provided, then a new Qrel will be constructed.")
    parser.add_argument('--eval_output_path', '-e', type=str, help="(Optional) .json path to save results in. If no path provided, results will simply be printed.")
    parser.add_argument('--qrel_output_path', '-qo', type=str, help="(Optional) .json path to save Qrel to.")
    parser.add_argument('--dataset_divides', '-z', type=int, default=1, help="Defaults to 1 batch. If evaluating a large test set, you may need to divide it up to prevent memory errors using this arg.")

    return parser.parse_args()

def main():
    args = get_args()

    match args.pipe.lower():
        case 'data':
            if not (args.metadata_path and args.images_path and args.splits_path):
                raise argparse.ArgumentError(None, "Missing required arguments for data pipe. Usage: --pipe data -m <metadata_path> -i <images_path> -s <save_splits_path>.")
            train_split, val_split, test_split = process_data(args.metadata_path, args.images_path, validate_data=True)
            if not os.path.exists(args.splits_path):
                os.mkdir(args.splits_path)
            with open(os.path.join(args.splits_path, 'train_split.npy'), 'wb') as f1:
                np.save(f1, train_split)
            with open(os.path.join(args.splits_path, 'val_split.npy'), 'wb') as f2:
                np.save(f2, val_split)
            with open(os.path.join(args.splits_path, 'test_split.npy'), 'wb') as f3:
                np.save(f3, test_split)

        case 'finetune':
            if not (args.splits_path and args.corrupted and args.c_input_path and args.c_output_path):
                raise argparse.ArgumentError(None, f"Missing required arguments for finetune pipe. Usage: --pipe finetune -s <splits_path> -x <corrupted_files_path> -c <checkpoint_input_path> -o <checkpoint_output_path>")
            if os.path.exists(args.c_output_path):
                raise FileExistsError(f"Designated output folder '{args.c_output_path}' already exists. Please delete it or provide a different output folder name for c_output_path.")
            os.mkdir(args.c_output_path)
            if args.distributed:
                os.environ["MASTER_ADDR"] = "localhost"
                os.environ["MASTER_PORT"] = "12354"
                world_size = torch.cuda.device_count()
                assert world_size >= 2, f"Distributed requires at least 2 GPUs to run, but got {world_size}"
                mp.spawn(finetune, args=(args.distributed, args.splits_path, args.corrupted, args.c_input_path, args.c_output_path, 15, args.batch_size, world_size), nprocs=world_size, join=True)
            else:
                finetune(0, args.distributed, args.splits_path, args.corrupted, args.c_input_path, args.c_output_path, batch_size=args.batch_size)
            
        case 'evaluate':
            metrics = ['precision@1', 'mrr'] # Can modify desired metrics here. Reference Ranx library to get list of valid metric names.
            if not (args.c_input_path and args.test_split_path and args.corrupted):
                raise argparse.ArgumentError(None, f"Missing required arguments for evaluation pipe. Usage: --pipe evaluation -c <Long-CLIP checkpoint path> -t <test_split_path> -x <corrupted_files_path>")
            evaluate_model(args.c_input_path, args.test_split_path, args.corrupted, metrics, args.qrel_input_path, args.eval_output_path, args.qrel_output_path, args.return_mean, args.dataset_divides)

        case 'complete':
            if not (args.metadata_path and args.images_path):
                raise argparse.ArgumentError(None, "Missing required arguments for complete pipe. Usage: --pipe complete -m <metadata_path> -i <images_path>")
            train_split, val_split, test_split = process_data(args.metadata_path, args.images_path, validate_data=True)

if __name__ == "__main__":
    main() # Wrapped code in main function so that spawned processes don't re-run everything.