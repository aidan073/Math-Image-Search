from submodules.Long_CLIP.model import longclip
from src.experiment.prepare_data import process_data
from src.experiment.finetuner import finetune

import os
import torch
import argparse
import numpy as np
from PIL import Image

# will perform the following:
# prepare dataset
# finetune
# evaluate

parser = argparse.ArgumentParser(description="Full experiment pipeline. Data processing, finetuning, and evaluation.")
# shared args
parser.add_argument('--pipe', '-p', type=str, choices=['data', 'finetune', 'evaluate', 'complete'], help="Which part of pipeline to run. Options are: data, finetune, evaluate, complete")
parser.add_argument('--splits_path', '-s', type=str, help="Folder path to save test/train/val splits (when using data pipe), or to load the splits from (when using finetune pipe).")

# data processing exclusive args
parser.add_argument('--metadata_path', '-m', type=str, help="Path to metadata tsv file.")
parser.add_argument('--images_path', '-i', type=str, help="Path to images folder.")

# finetune exclusive args
parser.add_argument('--distributed', '-d', action="store_true", help="Multi-gpu fine-tuning.")
parser.add_argument('--batch_size', '-b', type=int, default=30, help="Fine-tuning batch_size, defaults to 30.")
parser.add_argument('--corrupted', '-x', type=str, help="Path to the .txt file that contains missing or corrupted images (.txt generated by data pipe).")
parser.add_argument('--c_input_path', '-c', type=str, help="LongCLIP checkpoint path to finetune.")
parser.add_argument('--c_output_path', '-o', type=str, help="Folder path to save the checkpoints generated by finetune pipe.")

args = parser.parse_args()

match args.pipe.lower():
    case 'data':
        if not (args.metadata_path and args.images_path and args.splits_path):
            raise argparse.ArgumentError(None, "Missing required arguments for data pipe. Usage: --pipe data -m <metadata_path> -i <images_path> -s <save_splits_path>.")
        train_split, val_split, test_split = process_data(args.metadata_path, args.images_path, validate_data=True)
        if not os.path.exists(args.splits_path):
            os.mkdir(args.splits_path)
        with open(os.path.join(args.splits_path, 'train_split.npy'), 'wb') as f1:
            np.save(f1, train_split)
        with open(os.path.join(args.splits_path, 'val_split.npy'), 'wb') as f2:
            np.save(f2, val_split)
        with open(os.path.join(args.splits_path, 'test_split.npy'), 'wb') as f3:
            np.save(f3, test_split)

    case 'finetune':
        if not (args.splits_path and args.corrupted and args.c_input_path and args.c_output_path):
            raise argparse.ArgumentError(None, "Missing required arguments for finetune pipe. Usage: --pipe finetune -s <splits_path> -x <corrupted_files_path> -c <checkpoint_input_path> -o <checkpoint_output_path> [--distributed]")
        if os.path.exists(args.c_output_path):
            raise FileExistsError("Designated output folder {args.c_output_path} already exists. Please delete it or provide a different output folder name for c_output_path.")
        os.mkdir(args.c_output_path)
        finetune(args.distributed, args.splits_path, args.corrupted, args.c_input_path, args.c_output_path, batch_size=args.batch_size)
        

    case 'evaluate':
        pass

    case 'complete':
        if not (args.metadata_path and args.images_path):
            raise argparse.ArgumentError(None, "Missing required arguments for complete pipe. Usage: --pipe complete -m <metadata_path> -i <images_path>")
        train_split, val_split, test_split = process_data(args.metadata_path, args.images_path, validate_data=True)

# device = "cuda" if torch.cuda.is_available() else "cpu"
# model, preprocess = longclip.load("./checkpoints/longclip-B.pt", device=device)

# text = longclip.tokenize(["A man is crossing the street with a red car parked nearby.", "A man is driving a car in an urban scene."]).to(device)
# image = preprocess(Image.open("./img/demo.png")).unsqueeze(0).to(device)

# with torch.no_grad():
#     image_features = model.encode_image(image)
#     text_features = model.encode_text(text)
    
#     logits_per_image = image_features @ text_features.T
#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()

# print("Label probs:", probs) 