from Long_CLIP.model.longclip import tokenize, load

import os
import torch
import argparse
import numpy as np
from ranx import Run
from PIL import Image
from torchvision.transforms import Compose

def parse_args():
    parser = argparse.ArgumentParser(description="Search with a Long-CLIP checkpoint.")
    parser.add_argument('--results_path', '-r', type=str, help="Path to save search results in.")
    parser.add_argument('--c_input_path', '-c', type=str, help="Long-CLIP checkpoint path to finetune.")
    parser.add_argument('--test_split_path', '-t', type=str, help="Path to test split .npy file (generated by prepare_data).")

    return parser.parse_args()

def longclip_search(checkpoint_path:str, test_split_path:dict, output_path:str=None)->Run:
    """
    Use Long-CLIP checkpoint for text to image retrieval

    Args:
        checkpoint_path: Path to Long-CLIP checkpoint file
        test_split_path: Path to test split json file
        output_path (optional): Save run to this path

    Returns:
        Ranx Run
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = load(checkpoint_path, device=device)
    texts, images = _load_test(test_split_path, device, tokenize, preprocess)

    with torch.no_grad():
        text_embeddings = model.encode_text(texts)
        image_embeddings = model.encode_image(images)
    
    text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)
    image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True)
    logits_per_caption = text_embeddings @ image_embeddings.T

    return _construct_run(logits_per_caption, "LongCLIP_retrieval", output_path)

def _load_test(test_split_path:str, device:str, tokenize:function, preprocess:Compose)->tuple[list, list]:
    test_split = np.load(test_split_path)
    texts = []
    images = []
    for sample in test_split_path:
        texts.append(tokenize(sample[1], truncate=True).to(device))
        images.append(preprocess(Image.open(sample[2])).unsqueeze(0).to(device))

    return texts, images

def _construct_run(true_captions:list, logits_per_caption:list, run_name:str, output_path:str=None, top_n:int=100):
    caption_ids = list(true_captions.keys()) # for index to id conversion
    run_dict = {}
    for idx, (id, _) in enumerate(true_captions.items()):
        top_matching_indices = logits_per_caption[idx, :].argsort(dim=0, descending=True)[:top_n]
        values = logits_per_caption[idx, :][top_matching_indices]
        run_dict[id] = {}
        for key_idx, value in zip(top_matching_indices, values):
            run_dict[id][caption_ids[key_idx]] = value.item()
    run = Run(run_dict, run_name)
    if output_path:
        run.save(output_path)
    return run

if __name__ == "__main__":
    args = parse_args()
    if not (args.results_path and args.c_input_path and args.test_split_path):
        raise argparse.ArgumentError(None, "Missing required arguments for searching. Usage: -r <path to save results in> -c <path to Long-CLIP checkpoint> -t <path to test split>.")

    if os.path.exists(args.results_path):
        response = input(f"Designated output file '{args.results_path}' already exists. Do you want to override it? (y/N): ").strip().lower()
        if response != "y" and response != "yes":
            raise FileExistsError(f"File '{args.results_path}' already exists. Aborting operation.")

    longclip_search(args.c_input_path, args.test_split_path, args.results_path)