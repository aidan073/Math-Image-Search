import os
import argparse
from ranx import Run

def parse_args():
    parser = argparse.ArgumentParser(description="Search with a Long-CLIP checkpoint.")
    parser.add_argument('--results_path', '-r', type=str, help="Path to save results in.")
    parser.add_argument('--c_input_path', '-c', type=str, help="LongCLIP checkpoint path to finetune.")
    parser.add_argument('--test_split_path', '-t', type=str, help="Path to test split .npy file (generated by prepare_data).")

    return parser.parse_args()

def longclip_search(checkpoint_path:str, test_split_path:dict, output_path:str=None)->Run:
    """
    Use longclip checkpoint for text to image retrieval

    Args:
        checkpoint_path: Path to longclip checkpoint file
        test_split_path: Path to test split json file
        output_path (optional): Save run to this path

    Returns:
        Ranx Run
    """
    true_captions = _load_test(test_split_path, generated_queries_path)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = longclip.load(checkpoint_path, device=device)

    caption_embeddings = torch.empty(len(true_captions), 768, device=device)
    image_embeddings = torch.empty(len(true_captions), 768, device=device)
    with torch.no_grad():
        for idx, sample in enumerate(true_captions.values()):
            caption_embeddings[idx, :] = model.encode_text(longclip.tokenize(sample["caption"], truncate=True).to(device))
            image_embeddings[idx, :] = model.encode_image(preprocess(Image.open(sample["file_path"])).unsqueeze(0).to(device))
    logits_per_caption = caption_embeddings @ image_embeddings.T
    # ENSURE PROPER NORMILZATION BEFORE DOT PRODUCT ^

    return _construct_run(true_captions, logits_per_caption, "LongCLIP_retrieval", output_path)

def _load_test(test_split_path:str):
    test_split = np.load(test_split_path)

def _construct_run(true_captions:list, logits_per_caption:list, run_name:str, output_path:str=None, top_n:int=100):
    caption_ids = list(true_captions.keys()) # for index to id conversion
    run_dict = {}
    for idx, (id, _) in enumerate(true_captions.items()):
        top_matching_indices = logits_per_caption[idx, :].argsort(dim=0, descending=True)[:top_n]
        values = logits_per_caption[idx, :][top_matching_indices]
        run_dict[id] = {}
        for key_idx, value in zip(top_matching_indices, values):
            run_dict[id][caption_ids[key_idx]] = value.item()
    run = Run(run_dict, run_name)
    if output_path:
        run.save(output_path)
    return run

if __name__ == "__main__":
    args = parse_args()
    if not (args.results_path and args.c_input_path and args.test_split_path):
        raise argparse.ArgumentError(None, "Missing required arguments for searching. Usage: -r <path to save results in> -c <path to Long-CLIP checkpoint> -t <path to test split>.")

    if os.path.exists(args.results_path):
        response = input(f"Designated output file '{args.results_path}' already exists. Do you want to override it? (y/N): ").strip().lower()
        if response != "y" and response != "yes":
            raise FileExistsError(f"File '{args.results_path}' already exists. Aborting operation.")

    longclip_search(args.c_input_path, args.test_split_path, args.results_path)